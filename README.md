# Repository Summary Service
A tiny FastAPI service that, given a public GitHub repository URL, returns a concise JSON summary of the project.
## ðŸ“¦ Chosen Model
**Qwen/Qwen3â€‘Coderâ€‘30Bâ€‘A3Bâ€‘Instruct** â€“ a powerful instructionâ€‘tuned LLM that works well for codeâ€‘centric tasks.
## ðŸš€ Setup & Run (Fresh Machine)
Prerequisite: Python 3.14+ installed and added to your PATH.
1. **Clone the repository**

``` bash
   git clone <repositoryâ€‘url>
   cd <repositoryâ€‘directory>
```

2. **Create a virtual environment**

``` bash
   python -m venv venv
   # Windows
   venv\Scripts\activate
   # macOS / Linux
   source venv/bin/activate
```

3. **Install dependencies**

``` bash
   pip install --upgrade pip
   pip install -r requirements.txt
```

The requirements.txt contains:

``` 
   fastapi
   uvicorn
   requests
   pydantic
   openai
```

4. **Set the API key**
The service talks to the Nebius LLM endpoint; you need an API key.
``` bash
   set NEBIUS_API_KEY=YOUR_KEY            # Windows
   export NEBIUS_API_KEY=YOUR_KEY         # macOS / Linux
```

5. **Start the server**

``` bash
   python main.py
   # or, equivalently
   uvicorn main:app --host 0.0.0.0 --port 8000
```

6. **Use the API**
 - Hello endpoint (sanity check)
``` 
     GET http://localhost:8000/hello
```

 - Summarize a GitHub repo

``` bash
     curl -X POST http://localhost:8000/summarize \
          -H "Content-Type: application/json" \
          -d '{"github_url": "https://github.com/owner/repo"}'
```

The response will be a JSON object with three fields:

``` json
   {
     "summary": "...",
     "technologies": ["Python", "FastAPI", "uvicorn", "..."],
     "structure": "..."
   }
```

 
## ðŸ“‚ How Repository Contents Are Handled

1. **Tree Retrieval** â€“ The service calls the GitHub API (`/git/trees/main?recursive=1`) to obtain a flat list of every file (blob) in the default branch.
2. **File Selection** â€“
 - The complete fileâ€‘list is sent to the firstâ€‘look LLM prompt.
 - The model returns a short list of representative file paths (entry points, core modules, configuration files, etc.).
 - This step keeps the downstream processing lightweight by focusing only on the most informative files.
3. **Content Fetching** â€“ For each selected path, the raw file is downloaded from the `raw.githubusercontent.com` endpoint.
4. **Summarization** â€“
 - The fileâ€‘tree and the fetched file contents are fed to a second LLM prompt (JSONâ€‘summary).
 - The model produces a structured JSON summary (project purpose, technologies, layout).
5. *Whatâ€™s Skipped & Why*
 - Nonâ€‘code assets (images, binaries, large data files) â€“ they rarely help a textual summary and would waste bandwidth.
 - Files containing backâ€‘ticks (`) in their path â€“ the implementation explicitly filters these out, assuming they are markupâ€‘related or temporary artifacts.
 - Huge files â€“ the LLM prompt size limit forces us to ignore files whose content would push the request beyond the token budget.

 
The result is a fast, lowâ€‘overhead service that gives you a clear, humanâ€‘readable overview of any public Pythonâ€‘based repository.